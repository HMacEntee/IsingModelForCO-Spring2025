\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,margin=.75in,footskip=0.25in]{geometry}
\usepackage{setspace} 
\usepackage{braket}
\usepackage{amssymb}

\begin{document}

\begin{enumerate}
\item \textbf{Project Title:} Training Transformer Network to Find Ground State of Ising Models

\item \textbf{Investigator List:} Levy Lin, Holden Mac Entee, Yanglet Xiao-Yang Liu

\item \textbf{Project Description}
\begin{enumerate}
	\item \textbf{Motivation}
	\newline
	The Ising Model has been a prominent problem in physics since 1920. The model represents a mathematical explanation of ferromagnetism in statistical mechanics. The ground state, or alignment of spins in an Ising model to minimize energy, has many applications such as determining physical properties; for instance, magnetism or phase transitions. A related model, known as \textbf{spin glass}, is a magnetic state characterized by randomness. More specifically, the Edwards-Anderson model of spin glass organizes spins on a $d$-dimensional lattice considering only nearest neighbor interactions. The Hamiltonian for this spin system is given by: $H = - \sum_{<ij>}J_{ij}S_iS_j$, as the external magnetic field is assumed 0. This simplified Hamiltonian is related to graph MaxCut, defined as: $H(\sigma) = - \sum_{ij\in E(G)}J_{ij}\sigma_i\sigma_j$. MaxCut, and therefore the ground state of a desired Ising model, can be found using combinatorial optimization.

	\item \textbf{Key Methodology}
	\newline
		Finding the ground state of Ising models is an NP-Hard problem. We seek to employ a transformer network aided by reinforcement learning that will find the ground state of an Ising model in a quicker time than current state-of-the-art implementations, and improve the scalability of the problem.
	\begin{enumerate}
			
		\item \textbf{\underline{Task 1: Read 3 $\sim$ 5 references and find the associated datasts of Ising models.}}
		\newline Currently we have a dataset from Stanford that can be used to benchmark algorithms. We aim to find several more references containing datasets representing Ising models that we will test current implementations on and compare our solution to.
		\item \textbf{\underline{Task 2: Reproduce Professor Liu's codes and pipeline for baseline.}}  We will deploy Ising model solvers such as ECO-DQN, Gurobi, and MCPG to create a benchmark that we will compare Professor Liu's codes to.

		\item \textbf{\underline{Task 3: Implement a variant of transformer network and use RL algorithm to train}}
		\newline
		\textbf{\underline{it under Professor Liu's guidance.}} We will implement Professor Liu's transformer network variant and leverage reinforcement learning to generate results on the same datasets used with the aforementioned algorithms to compare the ability of Professor Liu's algorithm.

	\end{enumerate}

	\item \textbf{Expected Outcomes}
	
	\begin{enumerate}
		\item Beat existing solvers by 0.5\% on graphs ranging from 300 to 500 nodes.
		\item Scale our algorithm to large graph instances ranging from 1,000 to 3,000 nodes and outperform existing solvers by 2-3\%.
	\end{enumerate}

\end{enumerate}

\item \textbf{Estimated Project Timeline}
\begin{enumerate}
	\item 12/1/2024 $\sim$ 12/31/2024: Read papers; identify datasets to benchmark; set up solver and pipeline
	\item 1/1/2025 $\sim$ 2/28/2025: Outperform existing solvers on medium and large datasets
	\item 3/1/2025 $\sim$ 4/30/2025: Write paper to submit to NeurIPS 2025
\end{enumerate}

\end{enumerate}

\begin{thebibliography}{00}
\bibitem{b1}Carlson, C., Davies, E., Kolla, A., \& Perkins, W. (2022). Computational thresholds for the fixed-magnetization Ising model. Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing (STOC 2022), 1459–1472. https://doi.org/10.1145/3519935.3520003
\bibitem{b2}Lucas, A. (2014). Ising formulations of many NP problems. Frontiers in Physics, 2(5). https://doi.org/10.3389/fphy.2014.00005
\bibitem{b3}Benlic, U., \& Hao, J.-K. (2013). Breakout Local Search for the Max-Cut problem. Engineering Applications of Artificial Intelligence, 26(3), 1162–1173. https://doi.org/10.1016/j.engappai.2012.09.001
\bibitem{b4}Chen, C., Chen, R., Li, T., Ao, R., \& Wen, Z. (2023). Monte Carlo policy gradient method for binary optimization. arXiv. https://arxiv.org/abs/2307.00783


\end{thebibliography}
\end{document}
